{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Dataset Sentiment Analysis\n",
    "\n",
    "#### Dataset - https://www.yelp.com/dataset/download\n",
    "\n",
    "#### Source code - https://github.com/iwiszhou/ML1010-final-project\n",
    "\n",
    "#### Group 10 - Haofeng Zhou - zhf85@my.yorku.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Yelp data-set. I would use this data-set to do a sentiment analysis.\n",
    "I would build a model to predict the review either positive or negative.\n",
    "This is a big data-set. Firstly, I would try to extra the review data and create a simple data-set,\n",
    "which only contain Review & Rating. After that, I would create a new column which is Class.\n",
    "Class column is either Positive or Negative. If Rating is grater than 3, I would mark Class to Positive. Otherwise,\n",
    "Negative. If I have more time at the end, I would introduce one more value to Class column which is Neutral ( when\n",
    "Rating is equal to 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/iwiszhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords if not existing\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set col to max width\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database name & tables' name\n",
    "db_name = \"yelp.db\"\n",
    "table_names = {\n",
    "    \"reviews\": \"reviews\",\n",
    "    \"clean_reviews\": \"clean_reviews\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_absolute_path(file_name):\n",
    "    return os.path.abspath('') + \"/\" + file_name\n",
    "\n",
    "\n",
    "# Save data to database\n",
    "def save_to_db(dataFrame, tableName):\n",
    "    con = sqlite3.connect(db_name)\n",
    "    dataFrame.to_sql(tableName, con)\n",
    "    con.close()\n",
    "\n",
    "\n",
    "# Get data (dataframe format) from database by table name\n",
    "def get_table_by_name(tableName):\n",
    "    con = sqlite3.connect(db_name)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM \" + tableName + \";\", con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Read data from file\n",
    "# NOTE - the data-set is too big. I have already to several time, my computer crash. So that, I would start with first\n",
    "# 10000 rows. I would increase the data-set size when training the model.\n",
    "def load_json():\n",
    "    filename = get_absolute_path('./yelp_dataset/review.json')\n",
    "    row_count = 0\n",
    "    row_limit = 10000\n",
    "    df = []\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            df.append(json.loads(line))\n",
    "            row_count = row_count + 1\n",
    "            if row_count > row_limit:\n",
    "                break\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 - Gather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Q1sbwvVQXV2734tPgoKj4Q' 'hG7b0MtEbXx5QzbzE6C_VA'\n",
      "  'ujmEBvifdJM6h6RLv4wQIg' 1.0 6 1 0\n",
      "  'Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.'\n",
      "  '2013-05-07 04:34:36']\n",
      " ['GJXCdrto3ASJOqKeVWPi6Q' 'yXQM5uF2jS6es16SJzNHfg'\n",
      "  'NZnhc2sEQy3RmzKTZnqtwQ' 5.0 0 0 0\n",
      "  \"I *adore* Travis at the Hard Rock's new Kelly Cardenas Salon!  I'm always a fan of a great blowout and no stranger to the chains that offer this service; however, Travis has taken the flawless blowout to a whole new level!  \\n\\nTravis's greets you with his perfectly green swoosh in his otherwise perfectly styled black hair and a Vegas-worthy rockstar outfit.  Next comes the most relaxing and incredible shampoo -- where you get a full head message that could cure even the very worst migraine in minutes --- and the scented shampoo room.  Travis has freakishly strong fingers (in a good way) and use the perfect amount of pressure.  That was superb!  Then starts the glorious blowout... where not one, not two, but THREE people were involved in doing the best round-brush action my hair has ever seen.  The team of stylists clearly gets along extremely well, as it's evident from the way they talk to and help one another that it's really genuine and not some corporate requirement.  It was so much fun to be there! \\n\\nNext Travis started with the flat iron.  The way he flipped his wrist to get volume all around without over-doing it and making me look like a Texas pagent girl was admirable.  It's also worth noting that he didn't fry my hair -- something that I've had happen before with less skilled stylists.  At the end of the blowout & style my hair was perfectly bouncey and looked terrific.  The only thing better?  That this awesome blowout lasted for days! \\n\\nTravis, I will see you every single time I'm out in Vegas.  You make me feel beauuuutiful!\"\n",
      "  '2017-01-14 21:30:33']\n",
      " ['2TzJjDVDEuAW6MR5Vuc1ug' 'n6-Gk65cPZL6Uz8qRm3NYw'\n",
      "  'WTqjgwHlXbSFevF32_DJVw' 5.0 3 0 0\n",
      "  \"I have to say that this office really has it together, they are so organized and friendly!  Dr. J. Phillipp is a great dentist, very friendly and professional.  The dental assistants that helped in my procedure were amazing, Jewel and Bailey helped me to feel comfortable!  I don't have dental insurance, but they have this insurance through their office you can purchase for $80 something a year and this gave me 25% off all of my dental work, plus they helped me get signed up for care credit which I knew nothing about before this visit!  I highly recommend this office for the nice synergy the whole office has!\"\n",
      "  '2016-11-09 20:09:03']\n",
      " ['yi0R0Ugj_xUx_Nek0-_Qig' 'dacAIZ6fTM6mqwW5uxkskg'\n",
      "  'ikCg8xy5JIg_NGPx-MSIDA' 5.0 0 0 0\n",
      "  \"Went in for a lunch. Steak sandwich was delicious, and the Caesar salad had an absolutely delicious dressing, with a perfect amount of dressing, and distributed perfectly across each leaf. I know I'm going on about the salad ... But it was perfect.\\n\\nDrink prices were pretty good.\\n\\nThe Server, Dawn, was friendly and accommodating. Very happy with her.\\n\\nIn summation, a great pub experience. Would go again!\"\n",
      "  '2018-01-09 20:56:38']\n",
      " ['11a8sVPMUFtaC7_ABRkmtw' 'ssoyf2_x0EQMed6fgHeMyQ'\n",
      "  'b1b1eb3uo-w561D0ZfCEiQ' 1.0 7 0 0\n",
      "  'Today was my second out of three sessions I had paid for. Although my first session went well, I could tell Meredith had a particular enjoyment for her male clients over her female. However, I returned because she did my teeth fine and I was pleased with the results. When I went in today, I was in the whitening room with three other gentlemen. My appointment started out well, although, being a person who is in the service industry, I always attend to my female clientele first when a couple arrives. Unbothered by those signs, I waited my turn. She checked on me once after my original 30 minute timer to ask if I was ok. She attended my boyfriend on numerous occasions, as well as the other men, and would exit the room without even asking me or looking to see if I had any irritation. Half way through, another woman had showed up who she was explaining the deals to in the lobby. While she admits timers must be reset half way through the process, she reset my boyfriends, left, rest the gentleman furthest away from me who had time to come in, redeem his deal, get set, and gave his timer done, before me, then left, and at this point my time was at 10 minutes. So, she should have reset it 5 minutes ago, according to her. While I sat there patiently this whole time with major pain in my gums, i watched the time until the lamp shut off. Not only had she reset two others, explained deals to other guest, but she never once checked on my time. When my light turned off, I released the stance of my mouth to a more relaxed state, assuming I was only getting a thirty minute session instead of the usual 45, because she had yet to come in. At this point, the teeth formula was not only burning the gum she neglected for 25 minutes now, but it began to burn my lips. I began squealing and slapping my chair trying to get her attention from the other room in a panic. I was in so much pain, that by the time she entered the room I was already out of my chair. She finally then acknowledged me, and asked if she could put vitamin E on my gum burn (pictured below). At this point, she has treated two other gums burns, while neglecting me, and I was so irritated that I had to suffer, all I wanted was to leave. While I waited for my boyfriend, she kept harassing me about the issue. Saying, \"well burns come with teeth whitening.\" While I totally agree, and under justifiable circumstances would not be as irritate, it could have easily been avoid if she had checked on me even a second time, so I could let her know. Not only did she never check on my physical health, she couldn\\'t even take two seconds to reset the timer, which she even admitted to me. Her accuse was that she was coming in to do it, but I had the light off for a solid two minutes before I couldn\\'t stand the pain. She admitted it should be reset every 15 minutes, which means for 25 minutes she did not bother to help me at all. Her guest in the lobby then proceeded to attack me as well, simply because I wanted to leave after the way I was treated. I also expected a refund for not getting a complete session today, due to the neglect, and the fact I won\\'t be returning for my last, she had failed to do that. She was even screaming from the door, and continued to until my boyfriend and I were down the steps. I have never in my life been more appalled by a grown woman\\'s behavior, who claims to be in the business for \"10 years.\" Admit your wrongs, but don\\'t make your guest feel unwelcome because you can\\'t do you job properly.'\n",
      "  '2018-01-30 23:07:38']]\n",
      "(10001, 9)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10001 entries, 0 to 10000\n",
      "Data columns (total 9 columns):\n",
      "review_id      10001 non-null object\n",
      "user_id        10001 non-null object\n",
      "business_id    10001 non-null object\n",
      "stars          10001 non-null float64\n",
      "useful         10001 non-null int64\n",
      "funny          10001 non-null int64\n",
      "cool           10001 non-null int64\n",
      "text           10001 non-null object\n",
      "date           10001 non-null object\n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 703.3+ KB\n",
      "None\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Get data from database or json file\n",
    "\n",
    "file_path = get_absolute_path(\"data.csv\")\n",
    "\n",
    "if os.path.isfile(file_path):\n",
    "    # Import csv\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "else:\n",
    "    df = load_json()\n",
    "    # Export to csv\n",
    "    df.to_csv(file_path, encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "# Top 5 records\n",
    "print(df.head().values)\n",
    "\n",
    "# Shape of dataframe\n",
    "print(df.shape)\n",
    "\n",
    "# View data information\n",
    "print(df.info())\n",
    "\n",
    "# Check na values\n",
    "print(df.isnull().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not NA value in this data-set. Next, let's create a new column to store our Class/Label value,\n",
    "which depennds on our 'stars' column, if 'stars' is great than 3, Class/Label is 1 - 'Positive'. Otherwise,\n",
    "it is 0 - 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class(label) column\n",
    "def get_class_label_value(row):\n",
    "    if row[\"stars\"] >= 3:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "review_file_path = get_absolute_path(\"review.csv\")\n",
    "\n",
    "if not os.path.isfile(review_file_path):\n",
    "    df[\"class\"] = df.apply(get_class_label_value, axis=1)\n",
    "\n",
    "    # Create new data frame\n",
    "    filter_df = df[['class', 'text']]\n",
    "    print(filter_df.head(1).values)\n",
    "    print(filter_df.shape[0])\n",
    "    print(filter_df.columns)\n",
    "\n",
    "    # Export to csv\n",
    "    filter_df.to_csv(review_file_path, encoding='utf-8', index=False)\n",
    "else:\n",
    "    # Import csv\n",
    "    filter_df = pd.read_csv(review_file_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 - Clean data / Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, let's balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_review_file_path = get_absolute_path(\"balance_review.csv\")\n",
    "\n",
    "if not os.path.isfile(balance_review_file_path):\n",
    "    # num of Positive record\n",
    "    print(filter_df.loc[filter_df[\"class\"] == 1].count())\n",
    "\n",
    "    # num of Negative record\n",
    "    print(filter_df.loc[filter_df[\"class\"] == 0].count())\n",
    "\n",
    "    # balance the data\n",
    "    balance_data_count = 10\n",
    "    n_df = filter_df.loc[filter_df[\"class\"] == 0][:balance_data_count]\n",
    "    # number of negative rows\n",
    "    print(\"Number of negative should be 100. Actual is \", len(n_df.loc[n_df[\"class\"] == 0]))\n",
    "    print(\"Number of positive should be 0. Actual is \", len(n_df.loc[n_df[\"class\"] == 1]))\n",
    "\n",
    "    p_df = filter_df.loc[filter_df[\"class\"] == 1][:balance_data_count]\n",
    "    # number of positive rows\n",
    "    print(\"Number of positive should be 100. Actual is \", len(p_df.loc[p_df[\"class\"] == 1]))\n",
    "    print(\"Number of negative should be 0. Actual is \", len(p_df.loc[p_df[\"class\"] == 0]))\n",
    "\n",
    "    # merge positive and negative together to become a balance data\n",
    "    filter_df = n_df.append(p_df)\n",
    "\n",
    "    filter_df.to_csv(balance_review_file_path, encoding='utf-8', index=False)\n",
    "else:\n",
    "    # Import csv\n",
    "    filter_df = pd.read_csv(balance_review_file_path, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secondly, we would use NLTK method to normalize our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Normalization - using NLTK\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "# filter_df[\"norm_text\"] = normalize_corpus(filter_df[\"text\"])\n",
    "#\n",
    "# # Check the result\n",
    "# print(filter_df[\"norm_text\"].describe())\n",
    "# print(filter_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result, the norm text is still have some words not fully converted to what we want.\n",
    "Such as, 'checked', 'costs', we expected those should stem correctly.\n",
    "Next, let's try library Spacy, which provide all lots of helper method for us to normalize our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's use Spacy to normolize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     20                                                                                                                                                                                                        \n",
      "unique    20                                                                                                                                                                                                        \n",
      "top       love shabu perspective fresh home limited bland water taste price miserable try good be clean skip sauce favor well judge quality place pot small hot selection expensive soup star appetite base quantity\n",
      "freq      1                                                                                                                                                                                                         \n",
      "Name: norm_text, dtype: object\n",
      "   class  \\\n",
      "0  0       \n",
      "\n",
      "                                                                                                                                                                                                           text  \\\n",
      "0  Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.   \n",
      "\n",
      "                                                                                                                            norm_text  \n",
      "0  pill service actually crook check total online charge 69 bill Avoid Hospital nerve avoid er 3 19 cents horrible cost hospital cent  \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "white_list_pos = [\"VERB\", \"PART\", \"NOUN\", \"ADJ\", \"ADV\"]\n",
    "\n",
    "\n",
    "def spacy_norm_text(text):\n",
    "    # tokenizing\n",
    "    doc = nlp(str(text))\n",
    "\n",
    "    ret_set = set()\n",
    "\n",
    "    # handle stop words, VERB, PART, ADJ, ADV and NOUN\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.text:  # remove stop words & empty string\n",
    "            if token.pos_ in white_list_pos:  # if token is in white list, taking lemma_ instead\n",
    "                ret_set.add(token.lemma_.lower().strip())\n",
    "\n",
    "    # handle PROPN\n",
    "    for token in doc.ents:\n",
    "        ret_set.add(token.text)\n",
    "\n",
    "    # convert to list\n",
    "    unique_list = list(ret_set)\n",
    "\n",
    "    return \" \".join(unique_list)\n",
    "\n",
    "\n",
    "norm_review_file_path = get_absolute_path(\"norm_review.csv\")\n",
    "\n",
    "if not os.path.isfile(norm_review_file_path):\n",
    "    filter_df[\"norm_text\"] = filter_df.apply(lambda row: spacy_norm_text(row[\"text\"]), 1);\n",
    "\n",
    "    # Export norm text to file\n",
    "    filter_df.to_csv(norm_review_file_path, encoding='utf-8', index=False)\n",
    "else:\n",
    "    # Import norm text data frame\n",
    "    filter_df = pd.read_csv(norm_review_file_path, encoding='utf-8')\n",
    "\n",
    "# Check the result\n",
    "print(filter_df[\"norm_text\"].describe())\n",
    "print(filter_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3 - Feature extraction from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using TF-IDF to convert text to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 222)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>15</th>\n",
       "      <th>25</th>\n",
       "      <th>30</th>\n",
       "      <th>45</th>\n",
       "      <th>80</th>\n",
       "      <th>about</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>want</th>\n",
       "      <th>watch</th>\n",
       "      <th>water</th>\n",
       "      <th>way</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.174245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095106</td>\n",
       "      <td>0.095106</td>\n",
       "      <td>0.087123</td>\n",
       "      <td>0.105399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075084</td>\n",
       "      <td>0.095106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105399</td>\n",
       "      <td>0.066093</td>\n",
       "      <td>0.075084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230908</td>\n",
       "      <td>0.255898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160466</td>\n",
       "      <td>0.182297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171843</td>\n",
       "      <td>0.155061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151956</td>\n",
       "      <td>0.172629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103324</td>\n",
       "      <td>0.117381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.123956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106828</td>\n",
       "      <td>0.135314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094035</td>\n",
       "      <td>0.106828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197965</td>\n",
       "      <td>0.119747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085305</td>\n",
       "      <td>0.108053</td>\n",
       "      <td>0.119747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316331</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094350</td>\n",
       "      <td>0.214372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130321</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.179586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          10       100        15        25        30        45        80  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.174245  0.000000  0.095106  0.095106  0.087123  0.105399  0.000000   \n",
       "2   0.000000  0.000000  0.230908  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.171843  0.155061  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.272400  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.123956  0.000000  0.000000  0.000000  0.123956  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.119747  0.000000  0.000000  0.197965  0.119747  0.000000   \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.146683  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.227598  0.000000  0.000000  0.252229   \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  0.135768  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.207825   \n",
       "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.179586  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       about  actually       add  ...      want     watch     water       way  \\\n",
       "0   0.000000  0.315025  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.000000  ...  0.075084  0.095106  0.000000  0.095106   \n",
       "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.218662  ...  0.000000  0.000000  0.000000  0.218662   \n",
       "5   0.164772  0.000000  0.000000  ...  0.117381  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.149959  0.000000  ...  0.106828  0.135314  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.262448  0.000000   \n",
       "8   0.000000  0.000000  0.000000  ...  0.085305  0.108053  0.119747  0.000000   \n",
       "9   0.177453  0.000000  0.160124  ...  0.126414  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.147129   \n",
       "11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.135768  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  ...  0.148051  0.000000  0.000000  0.000000   \n",
       "17  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        work     worth     write     wrong      year     years  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.105399  0.066093  0.075084  \n",
       "2   0.000000  0.230908  0.255898  0.000000  0.160466  0.182297  \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.151956  0.172629  \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.103324  0.117381  \n",
       "6   0.123956  0.000000  0.149959  0.000000  0.094035  0.106828  \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8   0.098982  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "10  0.000000  0.147129  0.000000  0.000000  0.000000  0.000000  \n",
       "11  0.208492  0.000000  0.000000  0.000000  0.316331  0.000000  \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "14  0.124371  0.000000  0.000000  0.000000  0.094350  0.214372  \n",
       "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "16  0.000000  0.187530  0.000000  0.000000  0.130321  0.000000  \n",
       "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "19  0.000000  0.000000  0.000000  0.304676  0.000000  0.000000  \n",
       "\n",
       "[20 rows x 222 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf = vectorizer.fit_transform(filter_df[\"norm_text\"].values)\n",
    "\n",
    "# convert to array\n",
    "tfidf = tfidf.toarray()\n",
    "print(tfidf.shape)  # 200 is our rows, 1186 is how many words\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# plt.figure(figsize=[20,4])\n",
    "# _ = plt.show(tfidf)\n",
    "\n",
    "pd.DataFrame(tfidf, columns=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4 - Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 0]\n",
      "[0 0 1 1 0 1]\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iwiszhou/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tfidf  # the features we want to analyze\n",
    "y = filter_df['class'].values  # the labels, or answers, we want to test against\n",
    "\n",
    "# split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "predict_ret = model.predict_proba(X_test)\n",
    "\n",
    "# convert to Positive and Negative\n",
    "y_predict = np.array([int(p[1] > 0.5) for p in predict_ret])\n",
    "\n",
    "# accuracy\n",
    "print(y_predict)\n",
    "print(y_test)\n",
    "print(np.sum(y_test == y_predict) / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
